{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69b9e520-cb1f-4346-81ae-85e990bda9cc",
   "metadata": {},
   "source": [
    "# Deep Learning Applications: Laboratory #3\n",
    "\n",
    "In this laboratory session we worked to understand some implementations of Deep Reinforcement Learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4229ac23-970f-4dd7-938e-80c382c11fe4",
   "metadata": {},
   "source": [
    "## Exercise 1: `REINFORCE` Implementation \n",
    "\n",
    "In this exercise we start with a `REINFORCE` implementation and train an agent to balance a pole on a cart in the CartPole-v1 environment from OpenAI Gym.\n",
    "\n",
    "* Policy Network (PolicyNet) - we used a simple feedforward neural network with one hidden layer that outputs a probability distribution over actions using softmax.\n",
    "\n",
    "* Action Selection (select_action) uses the policy network to sample an action.\n",
    "\n",
    "* Computing Returns (compute_returns). It computes rewards to estimate how valuable past actions were.\n",
    "\n",
    "* Running an Episode (run_episode) simulates one complete run of the environment executing actions based on the policy, collects rewards.\n",
    "\n",
    "Training with REINFORCE uses Monte Carlo Policy Gradient to update the policy network evaluating performance every eval_interval episodes.\n",
    "At first, the agent will take random actions. But iver time, the policy network improves by assigning higher probabilities to better actions. At some point the agent should be able to balance the pole longer and achieve higher rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c9040f-7629-4fc2-ab44-c288ca0f3715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae3090a-9458-42a7-a4f7-27c716b78632",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(env.observation_space.shape[0], 16)\n",
    "        self.fc2 = nn.Linear(16, env.action_space.n)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, s):\n",
    "        s = F.relu(self.fc1(s))\n",
    "        s = F.softmax(self.fc2(s), dim=-1)\n",
    "        return s\n",
    "    \n",
    "def select_action(env, obs, policy):\n",
    "    dist = Categorical(policy(obs))\n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "    return action.item(), log_prob.reshape(1)\n",
    "\n",
    "def compute_returns(rewards, gamma=0.99):\n",
    "   return np.flip(np.cumsum([gamma**(i+1)*r for (i, r) in enumerate(rewards)][::-1]), 0).copy()\n",
    "\n",
    "def run_episode(env, policy, maxlen=500):\n",
    "    observations = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "\n",
    "    (obs, info) = env.reset()\n",
    "    for i in range(maxlen):\n",
    "        obs = torch.tensor(obs)\n",
    "        (action, log_prob) = select_action(env, obs, policy)\n",
    "        observations.append(obs)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        \n",
    "        (obs, reward, term, trunc, info) = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        if term or trunc:\n",
    "            break\n",
    "    return (observations, actions, torch.cat(log_probs), rewards)\n",
    "\n",
    "def reinforce(policy, env, env_render = None, gamma = 0.99, num_episodes = 10, eval_interval = 100, eval_episodes = 10, standardize_returns = True):\n",
    "    opt = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "    running_rewards = [0.0]\n",
    "\n",
    "    avg_rewards = []\n",
    "\n",
    "    policy.train()\n",
    "    for episode in range(num_episodes):\n",
    "        (observations, actions, log_probs, rewards) = run_episode(env, policy)\n",
    "        returns = torch.tensor(compute_returns(rewards, gamma), dtype=torch.float32)\n",
    "        running_rewards.append(0.05 * returns[0].item() + 0.95 * running_rewards[-1])\n",
    "\n",
    "        if standardize_returns:\n",
    "            returns = (returns - returns.mean()) / returns.std()\n",
    "        \n",
    "        targets = returns\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss = (-log_probs * targets).mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if (episode+1) % eval_interval == 0:\n",
    "            rewards = [] \n",
    "            policy.eval()\n",
    "            for ep in range(eval_episodes):\n",
    "                _, _, _, r = run_episode(env, policy)\n",
    "                rewards.append(np.sum(r))\n",
    "            print(f\"Episode {episode+1}, average reward: {np.mean(rewards)}\")\n",
    "            avg_rewards.append(np.mean(rewards))\n",
    "            policy.train()\n",
    "    policy.eval()\n",
    "    return running_rewards, avg_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58cda4e-69c0-4f39-ab93-f240b71ebef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_render = gym.make('CartPole-v1', render_mode='human')\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "(obs, info) = env.reset()\n",
    "print(\"Observation: \",obs)\n",
    "print(\"Observation shape: \", obs.shape)\n",
    "print(\"Observation space: \",env.observation_space)\n",
    "print(\"Action space: \",env.action_space)\n",
    "\n",
    "policy = PolicyNet(env)\n",
    "\n",
    "num_episodes = 100\n",
    "eval_interval = 100\n",
    "eval_episodes = 10\n",
    "running_rewards, avg_rewards = reinforce(policy, env, env_render, num_episodes=num_episodes, eval_interval=eval_interval, eval_episodes=eval_episodes)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(running_rewards)\n",
    "plt.title('Running rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(eval_interval, num_episodes+1, eval_interval), avg_rewards)\n",
    "plt.title('Average rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31494b5c-9cbd-467e-a2d4-91ce55c3a573",
   "metadata": {},
   "source": [
    "We trained the policy using reinforce() for 600 episodes. Here are the results.\n",
    "\n",
    "![Alt Text](plots/cp_rr1.png) \n",
    "\n",
    "![Alt Text](plots/cp_ar1.png) \n",
    "\n",
    "It is quite clear that after only about 500 episodes the agent has been trained successfully, having a stable running reward."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39564ea3-c6b7-4b0d-99fa-919378986cd8",
   "metadata": {},
   "source": [
    "## Exercise 2: `REINFORCE` with a Value Baseline\n",
    "\n",
    "In this exercise we changed up REINFORCE to subtract a baseline from the target in the update equation in order to stabilize and  speed-up convergence still in the CartPole environment.\n",
    "\n",
    "We used a state-value function V(s) as a baseline subtracting it from the returns, stabilizing learning.\n",
    "A separate ValueNet is used to approximate V(s), so we trained a value network with the policy network optimizing it separately using.\n",
    "\n",
    "Afterwards, we want to compare the results of the two runs with the same number of episodes for training to observe improvements in stability and convergence speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81277e76-45f2-4088-be46-7f48d8e7f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c8664-536d-4a74-ac23-4006dfed5fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(env.observation_space.shape[0], 16)\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, s):\n",
    "        s = F.relu(self.fc1(s))\n",
    "        s = self.fc2(s)\n",
    "        return s\n",
    "        \n",
    "def run_episode(env, policy, value_net, maxlen=500):\n",
    "    observations = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    values = []\n",
    "\n",
    "    (obs, info) = env.reset()\n",
    "    for i in range(maxlen):\n",
    "        obs = torch.tensor(obs)\n",
    "        (action, log_prob) = select_action(env, obs, policy)\n",
    "        value = value_net(obs)\n",
    "        observations.append(obs)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        \n",
    "        (obs, reward, term, trunc, info) = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        if term or trunc:\n",
    "            break\n",
    "    return (observations, actions, torch.cat(log_probs), rewards, values)\n",
    "    \n",
    "def reinforce(policy, value_net, env, env_render = None, gamma = 0.99, num_episodes = 10, eval_interval = 100, eval_episodes = 10, standardize_returns = True):\n",
    "    opt = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "    opt_value = optim.Adam(value_net.parameters(), lr=1e-2)\n",
    "    running_rewards = [0.0]\n",
    "\n",
    "    avg_rewards = []\n",
    "\n",
    "    policy.train()\n",
    "    for episode in range(num_episodes):\n",
    "        (observations, actions, log_probs, rewards, values) = run_episode(env, policy, value_net)\n",
    "        values = torch.tensor(values, dtype=torch.float32)\n",
    "        returns = torch.tensor(compute_returns(rewards, gamma), dtype=torch.float32)\n",
    "        running_rewards.append(0.05 * returns[0].item() + 0.95 * running_rewards[-1])\n",
    "\n",
    "        if standardize_returns:\n",
    "            returns = (returns - returns.mean()) / returns.std()\n",
    "        \n",
    "        targets = returns - values\n",
    "\n",
    "        opt.zero_grad()\n",
    "        opt_value.zero_grad()\n",
    "        loss = (-log_probs * targets).mean()\n",
    "        loss_value = F.mse_loss(returns, values)\n",
    "        loss_value.requires_grad = True\n",
    "        loss_value.backward()\n",
    "        opt_value.step()\n",
    "        loss += loss_value\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if (episode+1) % eval_interval == 0:\n",
    "            rewards = [] \n",
    "            policy.eval()\n",
    "            value_net.eval()\n",
    "            for ep in range(eval_episodes):\n",
    "                _, _, _, r, _ = run_episode(env, policy, value_net)\n",
    "                rewards.append(np.sum(r))\n",
    "            print(f\"Episode {episode+1}, average reward: {np.mean(rewards)}\")\n",
    "            avg_rewards.append(np.mean(rewards))\n",
    "            policy.train()\n",
    "            value_net.train()\n",
    "    policy.eval()\n",
    "    value_net.eval()\n",
    "    return running_rewards, avg_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533e084-2661-48cb-bdcd-e0e0c48da57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_render = gym.make('CartPole-v1', render_mode='human')\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "(obs, info) = env.reset()\n",
    "print(\"Observation: \",obs)\n",
    "print(\"Observation shape: \", obs.shape)\n",
    "print(\"Observation space: \",env.observation_space)\n",
    "print(\"Action space: \",env.action_space)\n",
    "\n",
    "policy = PolicyNet(env)\n",
    "value_net = ValueNet(env)\n",
    "\n",
    "num_episodes = 600\n",
    "eval_interval = 100\n",
    "eval_episodes = 10\n",
    "running_rewards, avg_rewards = reinforce(policy, value_net, env, env_render, num_episodes=num_episodes, eval_interval=eval_interval, eval_episodes=eval_episodes)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(running_rewards)\n",
    "plt.title('Running rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(eval_interval, num_episodes+1, eval_interval), avg_rewards)\n",
    "plt.title('Average rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06e0acc-894f-4a77-87e5-da0b72a89495",
   "metadata": {},
   "source": [
    "Here are the results of the training.\n",
    "\n",
    "![Alt Text](plots/cp_rr2.png) \n",
    "\n",
    "![Alt Text](plots/cp_ar2.png) \n",
    "\n",
    "It is quite clear that after only about 500 episodes the agent has been trained successfully, having a stable running reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678f3f48-7b48-4c0b-95f1-d64d3d2b036c",
   "metadata": {},
   "source": [
    "## Exercise 3: Solving Lunar Lander with `REINFORCE`\n",
    "\n",
    "In this exercise we implemented policy gradient reinforcement learning using the REINFORCE algorithm with a baseline value network to stabilize training. It is applied to the LunarLander-v3 environment from OpenAI Gym.\n",
    "\n",
    "The agent is trained on LunarLander-v3, a continuous-state, discrete-action environment where a lander must navigate to a landing pad.\n",
    "\n",
    "On the beginning we expect the agent to perform poorly as it explores the environment, improvng over time learning a better strategy, with higher rewards and more stable landing behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b503786-e6f4-40ae-a5a1-70e4b13d22c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4b7104-ec5b-4842-bc77-901bd2e23fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy, value_net, maxlen=1000):\n",
    "    observations = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    values = []\n",
    "\n",
    "    (obs, info) = env.reset()\n",
    "    for i in range(maxlen):\n",
    "        obs = torch.tensor(obs)\n",
    "        (action, log_prob) = select_action(env, obs, policy)\n",
    "        value = value_net(obs)\n",
    "        observations.append(obs)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        \n",
    "        (obs, reward, term, trunc, info) = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        if term or trunc:\n",
    "            break\n",
    "    return (observations, actions, torch.cat(log_probs), rewards, values)\n",
    "\n",
    "def reinforce(policy, value_net, env, env_render = None, gamma = 0.99, num_episodes = 10, eval_interval = 100, eval_episodes = 10, standardize_returns = True):\n",
    "    opt = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "    opt_value = optim.Adam(value_net.parameters(), lr=1e-2)\n",
    "    running_rewards = [0.0]\n",
    "\n",
    "    avg_rewards = []\n",
    "    avg_episode_lengths = []\n",
    "\n",
    "    policy.train()\n",
    "    for episode in range(num_episodes):\n",
    "        (observations, actions, log_probs, rewards, values) = run_episode(env, policy, value_net)\n",
    "        values = torch.tensor(values, dtype=torch.float32)\n",
    "        returns = torch.tensor(compute_returns(rewards, gamma), dtype=torch.float32)\n",
    "        running_rewards.append(0.05 * returns[0].item() + 0.95 * running_rewards[-1])\n",
    "\n",
    "        if standardize_returns:\n",
    "            returns = (returns - returns.mean()) / returns.std()\n",
    "        \n",
    "        targets = returns - values\n",
    "\n",
    "        opt.zero_grad()\n",
    "        opt_value.zero_grad()\n",
    "        loss = (-log_probs * targets).mean()\n",
    "        loss_value = F.mse_loss(returns, values)\n",
    "        loss_value.requires_grad = True\n",
    "        loss_value.backward()\n",
    "        opt_value.step()\n",
    "        loss += loss_value\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if (episode+1) % eval_interval == 0:\n",
    "            rewards = [] \n",
    "            policy.eval()\n",
    "            value_net.eval()\n",
    "            for ep in range(eval_episodes):\n",
    "                _, _, _, r, _ = run_episode(env, policy, value_net)\n",
    "                rewards.append(np.sum(r))\n",
    "            print(f\"Episode {episode+1}, average reward: {np.mean(rewards)}\")\n",
    "            avg_rewards.append(np.mean(rewards))\n",
    "            policy.train()\n",
    "            value_net.train()\n",
    "    policy.eval()\n",
    "    value_net.eval()\n",
    "    return running_rewards, avg_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3afbb9-ec85-460e-ac40-014c933cdbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_render = gym.make('LunarLander-v3', render_mode='human')\n",
    "env = gym.make('LunarLander-v3')\n",
    "\n",
    "(obs, info) = env.reset()\n",
    "print(\"Observation: \",obs)\n",
    "print(\"Observation shape: \", obs.shape)\n",
    "print(\"Observation space: \",env.observation_space)\n",
    "print(\"Action space: \",env.action_space)\n",
    "\n",
    "policy = PolicyNet(env)\n",
    "value_net = ValueNet(env)\n",
    "\n",
    "num_episodes = 1000\n",
    "eval_interval = 100\n",
    "eval_episodes = 10\n",
    "running_rewards, avg_rewards = reinforce(policy, value_net, env, env_render, num_episodes=num_episodes, eval_interval=eval_interval, eval_episodes=eval_episodes)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(running_rewards)\n",
    "plt.title('Running rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(eval_interval, num_episodes+1, eval_interval), avg_rewards)\n",
    "plt.title('Average rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70ee50b-d15d-4da7-aa50-a7d437061887",
   "metadata": {},
   "source": [
    "Here are the results of the training for 1000 episodes. \n",
    "\n",
    "![Alt Text](plots/ll-rr.png) \n",
    "\n",
    "![Alt Text](plots/ll-ar.png) \n",
    "\n",
    "After about 800 episodes we can see on the running rewards graph that the model has improved significantly but still the average reward was around 0 at best. It is also important to note that that is longer than the easier CartPole environment with worse results.\n",
    "\n",
    "In some different runs when training fot 2000 episodes the maximum average reward got as high as 12, the tendency was to become higher with more iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ac3d0a-3608-4156-90e0-c6a4bba97cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
