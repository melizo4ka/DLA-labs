{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b026827f-9337-427e-a932-7701189d99eb",
   "metadata": {},
   "source": [
    "# Deep Learning Applications: Laboratory #2\n",
    "\n",
    "In this laboratory we studied LLM and some of their uses. \n",
    "\n",
    "## Exercise 1: Basic GPT model\n",
    "For this exercise we implement a simple LLM that uses a tokenized dataset or text such as Dante's Inferno to produce a text similar in style and wording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fdceeb-1b71-44e4-a231-4c4e8cd280a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8894a6fc-8b6d-447a-b79a-27eb2a4c401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset:\n",
    "    def __init__(self, filename, train=True, train_size=0.9, block_size=8):\n",
    "        self.block_size = block_size\n",
    "        self.train = train\n",
    "        \n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            raw_data = f.read()\n",
    "        \n",
    "        self.tokens = sorted(set(raw_data))\n",
    "        self.num_tokens = len(self.tokens)\n",
    "        self.char2idx = {c: i for i, c in enumerate(self.tokens)}\n",
    "        self.idx2char = {i: c for c, i in self.char2idx.items()}\n",
    "        \n",
    "        split_idx = int(len(raw_data) * train_size)\n",
    "        raw_data = raw_data[:split_idx] if train else raw_data[split_idx:]\n",
    "        \n",
    "        self.data = torch.tensor(self.encode(raw_data), dtype=torch.long)\n",
    "    \n",
    "    def encode(self, text):\n",
    "        return [self.char2idx[c] for c in text]\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        return ''.join(self.idx2char[i] for i in indices)\n",
    "    \n",
    "    def get_batch(self, batch_size):\n",
    "        idx = torch.randint(0, len(self) - self.block_size, (batch_size,))\n",
    "        x = torch.stack([self.data[i:i+self.block_size] for i in idx])\n",
    "        y = torch.stack([self.data[i+1:i+self.block_size+1] for i in idx])\n",
    "        return x.to(device), y.to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d22371b-f99b-4885-9086-3acac253d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_dim, head_size, bias=False)\n",
    "        self.tril = torch.tril(torch.ones(embed_dim, embed_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k, q, v = self.key(x), self.query(x), self.value(x)\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) * (C ** -0.5)\n",
    "        attn_scores = attn_scores.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        attn_probs = self.dropout(attn_probs)\n",
    "        return attn_probs @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471ee662-27ee-4df7-9a21-91f42351f2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, head_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SelfAttentionHead(embed_dim, head_size, dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(num_heads * head_size, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.proj(torch.cat([h(x) for h in self.heads], dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc17bae-e9f9-49fc-b0f0-c871eebf656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08cba6d-b674-4bc0-a6ed-84130555bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        head_size = embed_dim // num_heads\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads, head_size)\n",
    "        self.ffwd = FeedForward(embed_dim)\n",
    "        self.ln1, self.ln2 = nn.LayerNorm(embed_dim), nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        return x + self.ffwd(self.ln2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc275063-9b47-40ae-a584-46a32cc2b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embed_dim, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Embedding(block_size, embed_dim)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(embed_dim, num_heads) for _ in range(num_layers)])\n",
    "        self.ln_f = nn.LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        x = self.embed(idx) + self.pos_embed(torch.arange(T, device=device))\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(self.ln_f(x))\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), targets.view(-1)) if targets is not None else None\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_tokens):\n",
    "        for _ in range(max_tokens):\n",
    "            idx_cond = idx[:, -self.pos_embed.num_embeddings:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            idx_next = torch.multinomial(F.softmax(logits[:, -1, :], dim=-1), 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c98af5-ff2b-45ba-b69b-a19a77ae370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, dataset, epochs=5000, eval_interval=500, batch_size=6):\n",
    "    wandb.init(project=\"dante-model\", config={\n",
    "    \"block_size\": block_size,\n",
    "    \"num_heads\": num_heads,\n",
    "    \"embed_dim\": embed_dim,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"learning_rate\": lr\n",
    "})\n",
    "    wandb.watch(model, log=\"all\")\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        if epoch % eval_interval == 0:\n",
    "            X, Y = dataset.get_batch(batch_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "            wandb.log({\"epoch\": epoch, \"loss\": loss.item()})\n",
    "        \n",
    "        X, Y = dataset.get_batch(batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        _, loss = model(X, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bac4c0-8399-483b-a106-95cc4c069e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "block_size=64\n",
    "num_heads=4\n",
    "embed_dim=128\n",
    "num_layers=6\n",
    "lr=3e-4\n",
    "\n",
    "ds_train = TextDataset(\"1ddcd09.txt\", train=True, block_size=block_size)\n",
    "model = GPTModel(ds_train.num_tokens, block_size=block_size, embed_dim=embed_dim, num_heads=num_heads, num_layers=num_layers).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "train_model(model, optimizer, ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14e1ed8-abf0-4a2c-8f02-b3de7571c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check an example of a generated code\n",
    "print(ds_train.decode(model.generate(torch.zeros((1, 1), dtype=torch.long, device=device), 500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47ded1e-1be1-4405-b968-e60cec367e26",
   "metadata": {},
   "source": [
    "We produced a couple of versions of the model to see how it evolves.\n",
    "With training for **1.000** epochs we received text such as:\n",
    "\n",
    "\"besti solcitomerso, que' rivant'aIffio.\n",
    "\n",
    "Marli,  mandima te lede che saffraste' ste.\n",
    "\n",
    "Obborda de di fuo, non setch'atto si\n",
    "  XVer eregla fic'io piu' du balchio sosto\n",
    "  mosco con fosti pere naddi il porge\n",
    "  fe' ridieglinti al e rpietta da che alu coi.\n",
    "\n",
    "Non de son danzi o sestartei quar zaveastro.\n",
    "\n",
    "L'a gio te sbenta, so tui e fesu osci.\n",
    "\n",
    "; luro in a vio` ch'ovessi' chia Frezzia,\n",
    "\n",
    "non  che abbuosiana e ducia chi me ssoppre>>.\n",
    "\n",
    "Tianteco e s'arla ior cae` ritaliora,\n",
    "  parsosero a tenindo e' olora\"\n",
    "\n",
    "This text visually is similar to Divina Commedia but when closely watching it has a lot of things that don't make sense such as punctuation and some words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c733f0-8461-45b3-b47a-3b3819700241",
   "metadata": {},
   "source": [
    "For **5.000** epochs we get something like:\n",
    "\n",
    "\"mi richio in ciel chiusto ' l'abbialio tanta,\n",
    "  qual esso more or pozzo i sei gento,\n",
    "\n",
    "di' voltrova si coneo>>, ch'a s'elli avessa\n",
    "  nom'io lamor Diegon con suoi rivolse\n",
    "\n",
    "Chi fron fonde addo' ora gi,\n",
    "  e prel Nascino uscimo a rice, ai, compagne\n",
    "  e voniziol non sono` in monestro:\n",
    "  e; che' semmo ingogno i parti>>.\n",
    "\n",
    "Io la bella cosa;\n",
    "  e 'l lanzo ma sua sigua li spatti.\n",
    "\n",
    "L'acque a Dio avina credera loce,\n",
    "  buon hanno del podo indio, andar s'ogne.\n",
    "\n",
    "E non la patica leodia, e ul Zabra:\n",
    "  dal biatto\"\n",
    "\n",
    "The wordings are much better even if not perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ba9963-6ea7-4967-a11a-d23902c7a1fa",
   "metadata": {},
   "source": [
    "## Exercise 2: Working with LLMs\n",
    "\n",
    "In this exercise we will see how to use the Hugging Face model and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4864956-81a7-4d7f-a4d4-b58a78462564",
   "metadata": {},
   "source": [
    "## Exercise 2.1: Text tokenization\n",
    "\n",
    "The key classes to work with when using GPT-2 for text generation are GPT2Tokenizer and GPT2LMHeadModel. \n",
    "The GPT2Tokenizer encoded raw text into sub-word tokens. These tokens are then mapped to integer IDs that GPT-2 can process. When working with text generation, it's important to include an important part that is the language modeling head. This head attaches to the final hidden layers of the architecture, enabling it to generate text by predicting the next token in a sequence based on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5b318e-d0db-4784-9c6c-47766044ff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ebc359-9086-46f4-a103-a8f6290c9f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "# sample input text\n",
    "input_text = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n",
    "\n",
    "# encode text into sub-word tokens\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "token_ids = inputs[\"input_ids\"]\n",
    "\n",
    "print(\"Input text:\", input_text)\n",
    "print(\"Length of input text (characters):\", len(input_text))\n",
    "print(\"Token IDs:\", token_ids)\n",
    "print(\"Length of encoded sequence (tokens):\", token_ids.size(1))\n",
    "\n",
    "# encode token IDs back to text\n",
    "decoded_text = tokenizer.decode(token_ids[0], skip_special_tokens=True)\n",
    "print(\"Decoded text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c9d9f0-ecf0-4f19-92ef-f5ff088ebc21",
   "metadata": {},
   "source": [
    "We get these lenghts for basic and encoded text:\n",
    "Length of input text (characters): 445\n",
    "Length of encoded sequence (tokens): 153\n",
    "\n",
    "Some of the reasons why is length is so much shorter in theory can be:\n",
    "* many short words map directly to single tokens\n",
    "* spaces and punctuation are handled efficiently\n",
    "  \n",
    "If the text contains rare words or complex structures, the token count would be higher relative to the character count."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee679af8-2923-481b-a552-7cc5ef1b95cb",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Generating Text\n",
    "\n",
    "In this exercise we instantiate a pre-trained GPT2LMHeadModel and use the innate generate() method to generate text given a prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c402ce-50d6-4d15-b81e-6516990bcbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers .mport GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc8b5a3-b41a-43dc-8f67-bc95b96ff6c0",
   "metadata": {},
   "source": [
    "We then will use a new function generate_text() that given an input and the model, will return a new phrase. Some of the parameters to be set in this function are:\n",
    "* do_sample: whether to use sampling or not, meaning greedy or probabilistic approach.\n",
    "* temperature: the creativity level, with lower temperature it becomes more deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cd56ad-51d4-4d78-93f8-3e98dcbbd3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "def generate_text(prompt, max_length=50, do_sample=False, temperature=1.0):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids, \n",
    "        max_length=max_length, \n",
    "        do_sample=do_sample, \n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ddd4c0-3538-4353-af03-6c9e4d04ac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The dinner is\"\n",
    "\n",
    "# text using greedy decoding\n",
    "greedy_text = generate_text(prompt, do_sample=False)\n",
    "print(\"\\nGreedy decoding:\")\n",
    "print(greedy_text)\n",
    "\n",
    "# text using sampling\n",
    "sampled_text = generate_text(prompt, do_sample=True, temperature=0.2)\n",
    "print(\"\\nSampled decoding (temperature=0.2):\")\n",
    "print(sampled_text)\n",
    "\n",
    "sampled_text = generate_text(prompt, do_sample=True, temperature=0.7)\n",
    "print(\"\\nSampled decoding (temperature=0.7):\")\n",
    "print(sampled_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1079e78-31b0-4b6c-b7d7-2c4a0100b195",
   "metadata": {},
   "source": [
    "The result phrases that we got are:\n",
    "\n",
    "*Greedy decoding*:\n",
    "The dinner is a bit of a mess, but I'm glad I did it. I'm glad I did it. I'm glad I did it. I'm glad I did it. I'm glad I did it. I'm glad I did\n",
    "\n",
    "*Sampled decoding (temperature=0.2)*:\n",
    "The dinner is a bit of a mess. I'm not sure if it's because I'm not a good cook or if I'm just not good at it. I'm not sure if I'm a good cook. I'm not sure if I\n",
    "\n",
    "*Sampled decoding (temperature=0.7)*:\n",
    "The dinner is hosted by the University of Ottawa's School of Family and Community Studies. The event is also attended by Canada's first-ever U.S. ambassador to Canada, William J. St. John, and the United States'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16587c2a-d5d3-4ec1-a7f3-8e7bd76188bb",
   "metadata": {},
   "source": [
    "We can see that when using the default settings of the generate function, so the greedy approach, our phrase repeats the same 5 or so words, such as \"I'm glad I did it.\" likely because it is often used in a sentence of this type. Another thing to note is that the phrase generated is always the same because the greedy behaviour does not have any randomness in it to change the possible outcome.\n",
    "\n",
    "The higher the *temperature* the better is the generated result, it is more creative and not as repetitive as it would be with small temperature, such as 0.2 in our example.\n",
    "\n",
    "If we want to generate good phrases that are connected and make sense together we should not use the greedy approach, instead we should sample to create new sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c9557d-dd7a-45d8-80ce-0f10bcbe9e02",
   "metadata": {},
   "source": [
    "# Exercise 3: Test classification using LLM\n",
    "\n",
    "In this exercise, we used a pre-trained Large Language Model DistilBERT for a Natural Language Processing task. Since DistilBERT provides a special class token in their output, we used it directly for classification. The goal is to select a moderately sized dataset, such as IMDb and train a logistic regression model to perform sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3479a58b-2339-4d78-af78-2afe7c661b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import wandb\n",
    "import time\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c9b7be-b3aa-43fe-b85b-df1e40551448",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "x_train = dataset['train']['text']\n",
    "x_test = dataset['test']['text']\n",
    "\n",
    "y_train = np.array(dataset['train']['label'])\n",
    "y_test = np.array(dataset['test']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdde9e1-408f-4eaf-a677-3849f2bda4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertModel.from_pretrained(model_name).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2aac05-7de8-47ec-8841-58669b2fe86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"imdb-feature-extraction\", name=\"distilbert-cpu\")\n",
    "\n",
    "def extract_features(texts, batch_size=4, max_length=32):\n",
    "    features = []\n",
    "    total_batches = len(texts) // batch_size\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        batch_start_time = time.time()\n",
    "\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "        features.append(cls_embeddings)\n",
    "\n",
    "        batch_time = time.time() - batch_start_time\n",
    "        wandb.log({\"batch_time (s)\": batch_time, \"batch_index\": i // batch_size})\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    wandb.log({\"total_extraction_time (s)\": total_time})\n",
    "\n",
    "    return np.vstack(features)\n",
    "\n",
    "train_features = extract_features(x_train)\n",
    "test_features = extract_features(x_test)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133dc26b-e3e1-4df9-a78b-d5b22af632a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(train_features, y_train)\n",
    "y_pred = clf.predict(test_features)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831c2537-c5af-4b3f-8646-1b4067b4b873",
   "metadata": {},
   "source": [
    "After training the model and the classifier we get Logistic Regression Accuracy of 0.72.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9505bf3-f684-4685-911b-8370c46029a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 5\n",
    "sample_indices = random.sample(range(len(x_test)), num_samples)\n",
    "\n",
    "print(\"\\nSample Predictions:\")\n",
    "for idx in sample_indices:\n",
    "    print(f\"Text: {x_test[idx][:200]}...\")\n",
    "    print(f\"Predicted Label: {y_pred[idx]} | Actual Label: {y_test[idx]}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc19db74-a786-4ab0-8d84-a13392986ba3",
   "metadata": {},
   "source": [
    "Using the trained model and classifier these are some of the sample predictions that we got:\n",
    "\n",
    "Text: Plot in a nutshell - Duchess (voice of Eva Gabor) is the well polished single mother cat of three little kittens. When their owner, the wealthy elderly woman known as Madame Adelaide, realizes that he...\n",
    "Predicted Label: 1 | Actual Label: 1\n",
    "\n",
    "Text: 2:37 is an intense and fascinating drama which has some similarities in tone and subject with films like Bully, Elephant and Kids (although, by my point of view, 2:37 is a superior film to those three...\n",
    "Predicted Label: 1 | Actual Label: 1\n",
    "\n",
    "Text: Set in 1962 Hong Kong (in turbulent times, as we are informed), this extremely intimate story of a failed romance between a two married people tied to their traditions manages to recall the essence of...\n",
    "Predicted Label: 1 | Actual Label: 1\n",
    "\n",
    "Text: I honestly don't know where to begin when reviewing a movie as pathetic as Ernest Goes to Africa. Aside from two or three good laughs dispersed throughout the film, there is nothing positive about thi...\n",
    "Predicted Label: 0 | Actual Label: 0\n",
    "\n",
    "Text: In my analysis of \"Trois couleurs: Blanc\" I wrote that its tone is much lighter than the tone of \"Trois couleurs: Bleu\". I think it's the same with this film. This time it's not because of a tragic co...\n",
    "Predicted Label: 1 | Actual Label: 1\n",
    "\n",
    "This means that in these cases the trained model was always correct, but the classifier's accuracy can be further improved with longer training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
