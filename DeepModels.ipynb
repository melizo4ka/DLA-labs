{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fecde31-e5a7-42c7-8841-2b33480d134b",
   "metadata": {},
   "source": [
    "# Deep Learning Applications: Laboratory #1\n",
    "\n",
    "In this first laboratory we worked with some elements of Deep Models. \n",
    "\n",
    "## Exercise 1: \n",
    "The scope of this exercise was to study a Multilayer Perceptron and the possibilities of it's performance on a relatively easy dataset MNIST.\n",
    "A Multilayer Perceptron (MLP) is a type of feedforward neural network consisting of multiple layers of neurons. It is composed of:\n",
    "* Input layer which receives input features.\n",
    "* Hidden layers - 1+ layers with neurons that apply transformations using weights, biases, and activation functions.\n",
    "* Output layer which produces final predictions for implemented tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acf1c6a-c3b2-4b36-ae81-56813934b72f",
   "metadata": {},
   "source": [
    "## Exercise 1.1: A baseline MLP for MNIST Classification\n",
    "\n",
    "In this exercise, we implemented a MLP to classify handwritten digits from the MNIST dataset. The steps for this exercise:\n",
    "\n",
    "1. Data Preparation - we loaded the MNIST dataset, which consists of grayscale images of handwritten digits (0â€“9) and split the dataset into training, validation, and test sets.  \n",
    "\n",
    "2. MLP Model Implementation - we defined a MLP.  \n",
    "\n",
    "3. Training the Model - we trained the model for multiple epochs using the training set, optimizing using Adam.  \n",
    "\n",
    "4. Model Evaluation - after each epoch, we would evaluate the model on the validation set and compute validation accuracy.\n",
    "\n",
    "5. Plot Performance Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74c17ff-d355-4d78-ba96-ed281f750f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from typing import List, Tuple\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daab415c-98c7-4fff-a94b-0fcab2e956de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_mnist_data(val_size: int = 5000):\n",
    "    # loading the MNIST dataset and splitting it\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    ds_train = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    ds_test = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    \n",
    "    I = np.random.permutation(len(ds_train))\n",
    "    ds_val = Subset(ds_train, I[:val_size])\n",
    "    ds_train = Subset(ds_train, I[val_size:])\n",
    "    \n",
    "    return ds_train, ds_val, ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6808abf8-80d8-46f3-9303-7cb3ee2d57de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this function is to train a model for a single epoch \n",
    "def train_epoch(model, dl, opt, epoch , device = 'cpu'):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for xs, ys in tqdm(dl, desc=f'Training epoch {epoch}', leave=True):\n",
    "        xs, ys = xs.to(device), ys.to(device)\n",
    "        opt.zero_grad()\n",
    "        logits = model(xs)\n",
    "        loss = F.cross_entropy(logits, ys)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # logging loss with wandb\n",
    "        wandb.log({\"Train_loss\": loss.item(), \"epoch\": epoch})\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "# this function is to evaluate the model\n",
    "def evaluate_model(model, dl, epoch, is_test = False,  device = 'cpu', use_wandb = False):\n",
    "    model.eval()\n",
    "    predictions, gts = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xs, ys in tqdm(dl, desc='Evaluating', leave=False):\n",
    "            xs, ys = xs.to(device), ys.to(device)\n",
    "            logits = model(xs)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            gts.append(ys.cpu())\n",
    "            predictions.append(preds.cpu())\n",
    "    \n",
    "    gts = torch.cat(gts).numpy()\n",
    "    predictions = torch.cat(predictions).numpy()\n",
    "    \n",
    "    acc = accuracy_score(gts, predictions)\n",
    "    report = classification_report(gts, predictions, zero_division=0, digits=3)\n",
    "    \n",
    "    # logging accuracy with wandb\n",
    "    wandb.log({\"Validation_accuracy\": acc})\n",
    "    \n",
    "    return acc, report\n",
    "\n",
    "\n",
    "# this function is to plot the loss curve and validation accuracy\n",
    "def plot_validation_curves(losses_and_accs):\n",
    "    losses, accs = zip(*losses_and_accs)\n",
    "    \n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(losses, marker='o', label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Average Training Loss per Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(accs, marker='o', label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Best Accuracy = {np.max(accs):.4f} @ epoch {np.argmax(accs) + 1}')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4f5a89-3842-4473-b104-2ed20385003f",
   "metadata": {},
   "source": [
    "### A basic MLP\n",
    "\n",
    "The MLP class constructs a feedforward neural network with one input layer, some hidden layers with ReLU activation, and an output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad0470-95e7-4610-ac1a-e11aa6513083",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, hidden_layers, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(hidden_layers)])\n",
    "        self.head = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        for layer in self.hidden_layers:\n",
    "            x = F.relu(layer(x))\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "def train_model(model, dl_train, dl_val, epochs, opt, device):\n",
    "    losses_and_accs = []\n",
    "    for epoch in range(epochs):\n",
    "        loss = train_epoch(model, dl_train, opt, epoch, device)\n",
    "        val_acc, _ = evaluate_model(model, dl_val, device)\n",
    "        losses_and_accs.append((loss, val_acc))\n",
    "    \n",
    "    plot_validation_curves(losses_and_accs)\n",
    "    return losses_and_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26c0ae2-3153-40c4-a581-abdc3852a259",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# hyperparameters\n",
    "epochs = 15\n",
    "lr = 0.0001\n",
    "batch_size = 128\n",
    "input_size = 28 * 28\n",
    "width = 16\n",
    "depth = 2\n",
    "\n",
    "wandb.init(project=\"basic-training\", config={\n",
    "    \"epochs\": epochs,\n",
    "    \"learning_rate\": lr,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"width\": width,\n",
    "    \"depth\": depth\n",
    "})\n",
    "\n",
    "ds_train, ds_val, ds_test = load_mnist_data()\n",
    "\n",
    "# creating dataloaders\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dl_val = torch.utils.data.DataLoader(ds_val, batch_size=batch_size, num_workers=4)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# instantiating model and optimizer\n",
    "model_mlp = MLP(input_size, width, depth, 10).to(device)\n",
    "optimizer = torch.optim.Adam(model_mlp.parameters(), lr=lr)\n",
    "\n",
    "losses_and_accs = train_model(model_mlp, dl_train, dl_val, epochs, optimizer, device)\n",
    "test_acc, test_report = evaluate_model(model_mlp, dl_test, device)\n",
    "print(f\"Test Accuracy: {test_acc}\\nTest Report:\\n{test_report}\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1046017f-c2f9-4021-b2dd-e118b1c32505",
   "metadata": {},
   "source": [
    "We trained this MLP for 15 epochs, to achieve training loss\tof 0.25 and validation accuracy\tof 0.92. The plots produced during training can be seen here.\n",
    "![Alt Text](plots/llm_mnist.png)\n",
    "\n",
    "We can say that we trained a MLP classifier that achieves reasonable accuracy on MNIST, it is possible to make greater improvement if training for more epochs, but this results demonstrate that on a simple dataset such as MNIST MLP's performance is more than enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d180096b-4082-4b07-9f6d-15881d541243",
   "metadata": {},
   "source": [
    "## Exercise 1.2: CNN with and without Residual connections\n",
    "\n",
    "In this exercise, we extended the previous process from a simple MLP to Convolutional Neural Networks but on a more difficult dataset such as CIFAR-10. The goal is to analyze how deeper CNN architectures affect performance and to explore the benefits of residual connections.\n",
    "\n",
    "* defined a CNN with multiple convolutional layers and ConvBlocks that apply ReLU activation and pooling operations to extract features from images.\n",
    "* trained the model for multiple epochs using the Adam optimizer.\n",
    "* monitored training loss and validation accuracy over time.\n",
    "\n",
    "We also wanted to compare models with different numbers of ConvBlocks to observe how increasing depth affects performance when using or not Residual Connections. This to show that deeper networks without residual connections do not always perform better due to vanishing gradients or overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e784ee87-122e-4603-a1de-143b44059ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10_data(val_size: int = 5000):\n",
    "    # loading the CIFAR-10 dataset and splitting it\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "    \n",
    "    ds_train = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    ds_test = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "    \n",
    "    I = np.random.permutation(len(ds_train))\n",
    "    ds_val = Subset(ds_train, I[:val_size])\n",
    "    ds_train = Subset(ds_train, I[val_size:])\n",
    "    \n",
    "    return ds_train, ds_val, ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ff92ec-5d2f-4d13-97c0-38bd5b1378a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, num=1, channels=8, size=3):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv2d(channels, channels, kernel_size=size, padding=(size-1)//2) \n",
    "            for _ in range(num)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return reduce(lambda f, g: lambda x: g(F.relu(f(x))), self.layers, lambda x: x)(x)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num=2, channels=8, size=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, channels, kernel_size=size, padding=(size-1)//2)\n",
    "        self.cblock1 = ConvBlock(num=num, channels=channels, size=size)\n",
    "        self.cblock2 = ConvBlock(num=num, channels=channels, size=2)\n",
    "        self.cblock3 = ConvBlock(num=num, channels=channels, size=2)\n",
    "        \n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.output = nn.Linear(channels, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.cblock1(x)\n",
    "        x = F.max_pool2d(x, 3, 2)\n",
    "        x = self.cblock2(x)\n",
    "        x = F.max_pool2d(x, 3, 2)\n",
    "        x = self.cblock3(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b1220-3ccc-45fd-93fe-b98d9a878ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# hyperparameters\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "batch_size = 128\n",
    "num_blocks = 3  # ConvBlocks in the CNN\n",
    "channels = 16\n",
    "size = 3\n",
    "\n",
    "ds_train, ds_val, ds_test = load_cifar10_data(val_size=5000)\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dl_val = DataLoader(ds_val, batch_size=batch_size, num_workers=4)\n",
    "dl_test = DataLoader(ds_test, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "model = CNN(num=num_blocks, channels=channels, size=size).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "wandb.init(project=\"cnn_cifar10\", config={\n",
    "    \"epochs\": epochs,\n",
    "    \"learning_rate\": lr,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_blocks\": num_blocks,\n",
    "    \"channels\": channels,\n",
    "    \"kernel_size\": size\n",
    "})\n",
    "\n",
    "train_model(model, dl_train, dl_val, epochs, optimizer, device)\n",
    "\n",
    "test_acc, test_report = evaluate_model(model, dl_test, device)\n",
    "print(f'Final test accuracy: {test_acc:.4f}')\n",
    "print(f'Test classification report:\\n{test_report}')\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c5ab35-0e27-4aab-add7-8eeb941f1b2f",
   "metadata": {},
   "source": [
    "We trained this CNN without residual connections over 10 epochs. The model used 3 ConvBlocks. The results that can be seen in the image show that we arrived at a value of training loss equal to 1.31 and validation accuracy of 0.56.\n",
    "\n",
    "![Alt Text](plots/cnn_cifar10.png)\n",
    "\n",
    "So with a deeper and better model such as CNN on a more difficult dataset in 10 epochs we coudn't achieve the same results as in the exercise above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7702b6-4db3-403b-a4a4-69ef5ed1bb6e",
   "metadata": {},
   "source": [
    "Next, we decided to modify the architecture to include residual connections, which help deeper networks retain gradient flow and learn more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73323f10-6ba8-4023-9d0b-59d7f5c17767",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        return F.relu(x + identity)\n",
    "\n",
    "class ResNetCNN(nn.Module):\n",
    "    def __init__(self, depth=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.residual_blocks = nn.ModuleList([\n",
    "            ResidualBlock(64) for _ in range(depth)\n",
    "        ])\n",
    "        self.output = nn.Linear(64, 10)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        for block in self.residual_blocks:\n",
    "            x = block(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.flatten(1)\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8d1779-31cd-48c2-8b7c-d957ced07091",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "batch_size = 128\n",
    "num_blocks = 3  # ResidualBlocks in the ResNetCNN\n",
    "channels = 16\n",
    "size = 4\n",
    "\n",
    "ds_train, ds_val, ds_test = load_cifar10_data(val_size=5000)\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dl_val = DataLoader(ds_val, batch_size=batch_size, num_workers=4)\n",
    "dl_test = DataLoader(ds_test, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "model = ResNetCNN(depth=num_blocks).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "wandb.init(project=\"resnet_cifar10\", config={\n",
    "    \"epochs\": epochs,\n",
    "    \"learning_rate\": lr,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_blocks\": num_blocks,\n",
    "    \"channels\": 64,\n",
    "    \"kernel_size\": size\n",
    "})\n",
    "\n",
    "train_model(model, dl_train, dl_val, epochs, optimizer, device)\n",
    "\n",
    "test_acc, test_report = evaluate_model(model, dl_test, device)\n",
    "print(f'Final test accuracy: {test_acc:.4f}')\n",
    "print(f'Test classification report:\\n{test_report}')\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b975b97-5956-4faa-91f9-390b37b393f1",
   "metadata": {},
   "source": [
    "Here we trained for 10 epochs, the model had 3 ResidualBlocks meaning it was deeper. After training with these parameters we have training loss of 0.75 and validation accuracy of 0.70.\n",
    "\n",
    "![Alt Text](plots/cnnresnet_cifar10.png)\n",
    "\n",
    "We can see that the results are considerably better, but it comes with a much higer computational cost.\n",
    "Nonetheless we have evidence that introducing architectural improvements such as residual connections better the performance of a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108a2160-3436-473c-b493-532ce89ec045",
   "metadata": {},
   "source": [
    "## Exercise 2: The effectiveness of Residual Connections\n",
    "Now we will use our two models (with and without residual connections) to try to understand why the residual versions of the networks learn more effectively.\n",
    "We will do this using gradient values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3559ff3e-0185-4f77-ba52-d5e5f37760c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gradient_flow(model, dataloader, device):\n",
    "    model.train()\n",
    "    gradient_norms = []\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)        \n",
    "        loss.backward()\n",
    "        \n",
    "        layer_norms = []\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                layer_norms.append(param.grad.norm().item())\n",
    "        gradient_norms.append(layer_norms)\n",
    "        \n",
    "        if batch_idx >= 10:\n",
    "            break\n",
    "    \n",
    "    return np.array(gradient_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e81d3d-e678-4b40-b8d7-62bbd17f9a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_blocks = 3\n",
    "channels = 16\n",
    "kernel_size = 3\n",
    "batch_size = 128\n",
    "\n",
    "plain_model = CNN(num=num_blocks, channels=channels, size=size).to(device)\n",
    "resnet_model = ResNetCNN(depth=num_blocks).to(device)\n",
    "\n",
    "ds_train, ds_val, ds_test = load_cifar10_data(val_size=5000)\n",
    "dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "plain_grads = analyze_gradient_flow(plain_model, dl_train, device)\n",
    "resnet_grads = analyze_gradient_flow(resnet_model, dl_train, device)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"CNN Gradient Norms\")\n",
    "plt.boxplot(plain_grads)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"ResNet CNN Gradient Norms\")\n",
    "plt.boxplot(resnet_grads)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGradient Statistics Comparison:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"CNN. Mean: {plain_grads.mean():.6f}, deviation: {plain_grads.std():.6f}\")\n",
    "print(f\"ResNet CNN. Mean: {resnet_grads.mean():.6f}, deviation: {resnet_grads.std():.6f}\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0338306c-1f6b-4ec6-aa24-892db4f8ae80",
   "metadata": {},
   "source": [
    "We achived plots such as in the image. And the values to compare between are:\n",
    "CNN. Mean: 0.011, deviation: 0.022\n",
    "ResNet CNN. Mean: 0.115, deviation: 0.095\n",
    "\n",
    "![Alt Text](plots/gradcomp.png)\n",
    "\n",
    "As we can see the mean gradient is significantly higher in the ResNet model. and the gradient variance is also higher, meaning gradients are more diverse and spread out.\n",
    "\n",
    "Residual connections help mitigate vanishing gradients, which is a major issue in deep networks. The plain CNN has a very low deviation, meaning the gradients are almost uniform and may fail to capture complex patterns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
